[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quarto CRC Book",
    "section": "",
    "text": "Preface\nThis is a Quarto book."
  },
  {
    "objectID": "index.html#software-conventions",
    "href": "index.html#software-conventions",
    "title": "Quarto CRC Book",
    "section": "Software conventions",
    "text": "Software conventions\n\n1 + 1\n\n2\n\n\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Quarto CRC Book",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nBlah, blah, blah…"
  },
  {
    "objectID": "OBESITAS.html#tujuan",
    "href": "OBESITAS.html#tujuan",
    "title": "1  ",
    "section": "1.1 1. Tujuan",
    "text": "1.1 1. Tujuan\nMemprediksi atau mengukur tingkat sebuah tipe obesitas di masyarakat dengan menggunakan model prediksi. Tingkat obesitas dibedakan menjadi tujuh tipe, yaitu sebagai berikut:\n\nInsufficient Weight\nNormal Weight\nOverweight Level I\nOverweight Level II\nObesity Type I\nObesity Type II\nObesity Type III\n\nTujuan lainnya adalah memberikan informasi lebih mengenai bahaya dari masing - masing tipe obesitas serta bagaimana cara menghindari penyakit obesitas tersebut, semua ini akan ditampilkan di dalam model prediksi."
  },
  {
    "objectID": "OBESITAS.html#fitur-yang-digunakan",
    "href": "OBESITAS.html#fitur-yang-digunakan",
    "title": "1  ",
    "section": "1.2 2. Fitur yang digunakan",
    "text": "1.2 2. Fitur yang digunakan\nPada prediksi tingkat obesitas diatas terdapat fitur - fitur yang digunakan untuk mendapatkan hasil prediksi, diantaranya :\n\nGender (Jenis Kelamin)\nAge (Usia):\nFamily (Riwayat Keluarga dengan Obesitas)\nFCVC (Frequency of consumption of vegetables)\nNCP (Number of main meals)\nCAEC (Consumption of food between meals)\nSmoke (Smoking habit)\nCH20 (Consumption of water daily)\nSCC (Calories consumption monitoring)\nFAF (Physical activity frequency)\nTUE (Time using technology devices)\nCALC (Use of a caloric calculator)\nMTRANS (Mode of transportation)\nHeight (Tinggi)\nWeight (Berat)\nFAVC (Frequent consumption of high caloric food)\n\n#\n\n\n\n\nData Understanding - - -\n\n\n\n\nInformasi yang bisa didapatkan di Data Understanding ini adalah : 1. Type Data Pada Semua Fitur 2. Deskripsi Dataset Beserta Fitur 3. Eksplorasi Data (Grafikan Fitur) 4. Proporsi Setiap Kelas 5. Mengidentifikasi Missing Value 6. Mengidentifikasi Data Outlier"
  },
  {
    "objectID": "OBESITAS.html#type-data-pada-semua-fitur",
    "href": "OBESITAS.html#type-data-pada-semua-fitur",
    "title": "1  ",
    "section": "1.3 1. Type Data Pada Semua Fitur",
    "text": "1.3 1. Type Data Pada Semua Fitur\nData statistik adalah dapat diartikan sebagai bagian data tunggal, data yang memiliki pesan informasi yang faktual, yang terekam untuk tujuan analisis. Maksudnya data statistik adalah, data yang dapat dijadikan sumber informasi. Dimana data ini berasal dari hasil penyajian, interpretasi dan hasil analisis data. Berikut adalah Jenis - Jenis Type Data yang digunakan untuk nanti mengidentifikasikan fitur dataset ini : 1. Data Nominal : Berbeda lagi dengan data nominal. Data nominal adalah data yang sering digunakan untuk menghitung hasil statistik kualitatif. Data nominal digunakan untuk membantu dalam memberikan label pada variabel tanpa memberikan nilai numerik. Dimana numerik dapat diganti dengan huruf, simbol, jenis kelamin atau menggunakan kata lain. 2. Data Rasio : Data Rasio memiliki nilai yang hampir sama dengan nilai interval. Jika data interval tidak memiliki “0” yang benar, maka pada data rasio memiliki nilai “0” mutlak. 3. Data Interval : Data Interval adalah data statistik yang diperoleh dari hasil skala pengukuran. Data interval ternyata termasuk ke dalam data data kontinu. Secara perhitungan, data interval dapat ditambahkan ataupun dikurangi, namun tidak dapat dikalikan, dibagi ataupun dihitung rasionya, karena tidak memiliki nilai “0” yang benar. 4. Data Ordinal : Adapun yang disebut dengan data ordinal, atau data yang yang sering digunakan untuk data kualitatif. Dari segi penyajian, data ordinal banyak digunakan dalam bentuk diagram lingkaran dan diagram batang, dan sering ditafsirkan menggunakan banyak alat visualisasi. Dalam data ordinal, penyajian data dapat dituliskan dalam banyak bentuk. Bisa ditulis dalam bentuk persentil, rentang interkuartil untuk meringkas data, dan ada juga yang menggunakan median dan mode.\n    ****\nBerikut Type data di setiap fitur di dataset :\n1. Gender (Jenis Kelamin): Ini adalah data nominal karena jenis kelamin adalah label kualitatif yang hanya mengidentifikasi kategori “Laki-laki” atau “Perempuan.” Tidak ada urutan atau peringkat numerik. 2. Age (Usia): Ini adalah data rasio karena usia diukur dalam tahun, dan Anda dapat melakukan operasi matematis seperti penambahan, pengurangan, perkalian, dan pembagian terhadap usia. Nilai “0” yang benar juga ada, misalnya, ketika seseorang berusia 0 tahun. 3. Family (Riwayat Keluarga dengan Obesitas): Ini adalah data nominal karena hanya memberi label “Ya” atau “Tidak” untuk mengidentifikasi keberadaan atau ketiadaan riwayat obesitas dalam keluarga. 4. FCVC (Frequency of consumption of vegetables): Ini adalah data interval karena meskipun Anda mengukur frekuensi konsumsi sayuran dalam bentuk angka, Anda tidak bisa mengalikan atau membagi nilai-nilai ini. Tidak ada nilai “0” yang benar. 5. NCP (Number of main meals): Ini adalah data interval karena ini mewakili jumlah makanan utama yang dikonsumsi dalam sehari dalam bentuk angka. Namun, tidak ada nilai “0” yang benar. 6. CAEC (Consumption of food between meals): Ini adalah data nominal karena memberi label kebiasaan makan di antara waktu makan utama dengan kategori seperti “Selalu,” “Sering,” “Kadang-kadang,” atau “Tidak pernah.” 7. Smoke (Smoking habit): Ini adalah data nominal karena hanya memberi label pada kebiasaan merokok dengan kategori “Ya” atau “Tidak.” 8. CH20 (Consumption of water daily): Ini adalah data interval karena Anda mengukur seberapa banyak air yang dikonsumsi dalam bentuk angka, meskipun tidak ada nilai “0” yang benar. 9. SCC (Calories consumption monitoring):Ini adalah data nominal karena hanya memberi label apakah seseorang memantau asupan kalori mereka dengan kategori “Ya” atau “Tidak.” 10. FAF (Physical activity frequency): Ini adalah data interval karena mencerminkan frekuensi berpartisipasi dalam aktivitas fisik selama hari kerja, meskipun tidak ada nilai “0” yang benar. 11. TUE (Time using technology devices): Ini adalah data interval karena mencerminkan jumlah waktu yang dihabiskan menggunakan perangkat teknologi, meskipun tidak ada nilai “0” yang benar. 12. CALC (Use of a caloric calculator): Ini adalah data nominal karena hanya memberi label penggunaan kalkulator kalori dengan kategori seperti “Selalu,” “Sering,” “Kadang-kadang,” atau “Tidak pernah.” 13. MTRANS (Mode of transportation): Ini adalah data nominal karena hanya memberi label pada alat transportasi yang digunakan dengan kategori seperti “Mobil,” “Sepeda Motor,” “Sepeda,” “Transportasi Umum,” atau “Berjalan.” 14. Height (Tinggi): Ini adalah data rasio karena diukur dalam sentimeter, dan Anda dapat melakukan operasi matematis seperti pengurangan dan pembagian terhadap tinggi. Nilai “0” yang benar ada, misalnya, ketika seseorang memiliki tinggi 0 cm (yang sangat jarang terjadi). 15. Weight (Berat): Ini juga adalah data rasio karena diukur dalam kilogram, dan Anda dapat melakukan operasi matematis seperti pengurangan dan pembagian terhadap berat. Nilai “0” yang benar ada, misalnya, ketika seseorang memiliki berat 0 kg (yang juga sangat jarang terjadi). 16. FAVC (Frequent consumption of high caloric food): Ini adalah data nominal karena hanya memberi label pada apakah seseorang sering mengonsumsi makanan tinggi kalori dengan kategori “Ya” atau “Tidak.”\n\nimport numpy as np\nimport pandas as pd\n\ndata = pd.read_csv('data-obesitas.csv')\ndata\n\n\n\n\n\n\n\n\nGender\nAge\nHeight\nWeight\nfamily_history_with_overweight\nFAVC\nFCVC\nNCP\nCAEC\nSMOKE\nCH2O\nSCC\nFAF\nTUE\nCALC\nMTRANS\nNObeyesdad\n\n\n\n\n0\nFemale\n21.000000\n1.620000\n64.000000\nyes\nno\n2.0\n3.0\nSometimes\nno\n2.000000\nno\n0.000000\n1.000000\nno\nPublic_Transportation\nNormal_Weight\n\n\n1\nFemale\n21.000000\n1.520000\n56.000000\nyes\nno\n3.0\n3.0\nSometimes\nyes\n3.000000\nyes\n3.000000\n0.000000\nSometimes\nPublic_Transportation\nNormal_Weight\n\n\n2\nMale\n23.000000\n1.800000\n77.000000\nyes\nno\n2.0\n3.0\nSometimes\nno\n2.000000\nno\n2.000000\n1.000000\nFrequently\nPublic_Transportation\nNormal_Weight\n\n\n3\nMale\n27.000000\n1.800000\n87.000000\nno\nno\n3.0\n3.0\nSometimes\nno\n2.000000\nno\n2.000000\n0.000000\nFrequently\nWalking\nOverweight_Level_I\n\n\n4\nMale\n22.000000\n1.780000\n89.800000\nno\nno\n2.0\n1.0\nSometimes\nno\n2.000000\nno\n0.000000\n0.000000\nSometimes\nPublic_Transportation\nOverweight_Level_II\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2106\nFemale\n20.976842\n1.710730\n131.408528\nyes\nyes\n3.0\n3.0\nSometimes\nno\n1.728139\nno\n1.676269\n0.906247\nSometimes\nPublic_Transportation\nObesity_Type_III\n\n\n2107\nFemale\n21.982942\n1.748584\n133.742943\nyes\nyes\n3.0\n3.0\nSometimes\nno\n2.005130\nno\n1.341390\n0.599270\nSometimes\nPublic_Transportation\nObesity_Type_III\n\n\n2108\nFemale\n22.524036\n1.752206\n133.689352\nyes\nyes\n3.0\n3.0\nSometimes\nno\n2.054193\nno\n1.414209\n0.646288\nSometimes\nPublic_Transportation\nObesity_Type_III\n\n\n2109\nFemale\n24.361936\n1.739450\n133.346641\nyes\nyes\n3.0\n3.0\nSometimes\nno\n2.852339\nno\n1.139107\n0.586035\nSometimes\nPublic_Transportation\nObesity_Type_III\n\n\n2110\nFemale\n23.664709\n1.738836\n133.472641\nyes\nyes\n3.0\n3.0\nSometimes\nno\n2.863513\nno\n1.026452\n0.714137\nSometimes\nPublic_Transportation\nObesity_Type_III\n\n\n\n\n2111 rows × 17 columns"
  },
  {
    "objectID": "OBESITAS.html#deskripsi-dataset-beserta-fitur",
    "href": "OBESITAS.html#deskripsi-dataset-beserta-fitur",
    "title": "1  ",
    "section": "1.4 2. Deskripsi Dataset Beserta Fitur",
    "text": "1.4 2. Deskripsi Dataset Beserta Fitur\nDataset ini mencakup data estimasi tingkat obesitas pada individu dari negara Meksiko, Peru dan Kolombia, berdasarkan kebiasaan makan dan kondisi fisiknya. Data tersebut berisi 17 atribut dan 2111 record, record tersebut diberi label dengan variabel kelas NObesity (Tingkat Obesitas), yang memungkinkan klasifikasi data menggunakan nilai : - Berat Badan Tidak Cukup ( Ini mengindikasikan bahwa seseorang memiliki BMI di bawah kisaran yang dianggap sehat. Orang dengan berat badan tidak cukup mungkin mengalami masalah kesehatan karena kekurangan gizi ) - Berat Badan Normal ( Ini mengindikasikan bahwa seseorang memiliki BMI dalam kisaran yang dianggap sehat. Ini adalah kategori yang diinginkan karena biasanya memiliki risiko lebih rendah terhadap penyakit terkait obesitas ) - Kegemukan Tingkat I( Seseorang dianggap mengalami obesitas tingkat I jika BMI mereka berada dalam rentang tertentu yang menunjukkan peningkatan berat badan yang berlebihan. Ini dapat berisiko terhadap masalah kesehatan seperti diabetes tipe 2 dan penyakit jantung ) - Kegemukan Tingkat II ( Ini menunjukkan tingkat obesitas yang lebih serius. Orang dengan obesitas tingkat II memiliki BMI yang lebih tinggi dan berisiko lebih tinggi terhadap masalah kesehatan serius ) - Obesitas Tipe I ( Ini adalah kategori obesitas yang lebih lanjut, menunjukkan peningkatan yang signifikan dalam berat badan dan risiko kesehatan yang lebih tinggi ) - Obesitas Tipe II ( Orang dengan obesitas tipe II memiliki BMI yang sangat tinggi dan berisiko serius terhadap berbagai penyakit, termasuk penyakit jantung, tekanan darah tinggi, dan diabetes ) - Obesitas Tipe III ( Ini adalah tingkat obesitas yang paling tinggi. Orang dengan obesitas tipe III memiliki BMI yang sangat ekstrem dan menghadapi risiko tinggi terhadap berbagai komplikasi kesehatan serius )\nKlasfikasi diatas bisa ditentukan karena terdapar 16 atribut yang digunakan dalam mengklasfikasi, yaitu : 1. Gender (Jenis Kelamin): Ini mengidentifikasi jenis kelamin seseorang, dan biasanya memiliki dua nilai: “Laki-laki” dan “Perempuan” 2. Age (Usia): Ini adalah usia seseorang dalam tahun. Atribut ini mencerminkan usia pengguna. 3. Family (Riwayat Keluarga dengan Obesitas): Ini mengukur apakah seseorang memiliki riwayat obesitas dalam keluarganya. Nilai-nilai umumnya adalah “Ya” jika ada riwayat obesitas dalam keluarga dan “Tidak” jika tidak. 4. FCVC (Frequency of consumption of vegetables): Ini mengukur seberapa sering seseorang mengonsumsi sayuran. Nilai-nilai umumnya adalah angka yang mencerminkan frekuensi konsumsi sayuran. Semakin tinggi nilai FCVC, semakin sering seseorang mengonsumsi sayuran. 5. NCP (Number of main meals): Ini adalah jumlah makanan utama yang dikonsumsi oleh seseorang dalam sehari. Ini adalah angka yang mencerminkan jumlah kali seseorang makan makanan utama dalam sehari. 6. CAEC (Consumption of food between meals): Ini mengukur kebiasaan konsumsi makanan di antara waktu makan utama. Ini bisa berupa “Selalu”, “Sering”, “Kadang-kadang”, atau “Tidak pernah”. 7. Smoke (Smoking habit): Ini mencerminkan apakah seseorang adalah perokok atau tidak. Nilainya bisa “Ya” jika perokok dan “Tidak” jika bukan. 8. CH20 (Consumption of water daily): Ini mengukur seberapa banyak air yang dikonsumsi seseorang setiap hari. 9. SCC (Calories consumption monitoring): Ini mengindikasikan apakah seseorang memantau asupan kalori mereka. Nilainya bisa “Ya” jika memantau dan “Tidak” jika tidak. 10. FAF (Physical activity frequency): Ini mengukur seberapa sering seseorang berpartisipasi dalam aktivitas fisik selama hari kerja. 11. TUE (Time using technology devices): Ini mencerminkan seberapa banyak waktu yang dihabiskan seseorang menggunakan perangkat teknologi. 12. CALC (Use of a caloric calculator): Ini mengindikasikan apakah seseorang menggunakan kalkulator kalori sebagai alat bantu kesehatan. Nilainya bisa “Selalu”, “Sering”, “Kadang-kadang”, atau “Tidak pernah”. 13. MTRANS (Mode of transportation): Ini mencerminkan alat transportasi yang digunakan seseorang. Ini bisa berupa “Mobil”, “Sepeda Motor”, “Sepeda”, “Transportasi Umum”, atau “Berjalan”. 14. Height (Tinggi): Tinggi seseorang dalam sentimeter. 15. Weight (Berat): Berat seseorang dalam kilogram. 16. FAVC (Frequent consumption of high caloric food): Ini mengindikasikan apakah seseorang sering mengonsumsi makanan tinggi kalori. Nilainya bisa “Ya” jika sering mengonsumsi dan “Tidak” jika tidak.\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2111 entries, 0 to 2110\nData columns (total 17 columns):\n #   Column                          Non-Null Count  Dtype  \n---  ------                          --------------  -----  \n 0   Gender                          2111 non-null   object \n 1   Age                             2111 non-null   float64\n 2   Height                          2111 non-null   float64\n 3   Weight                          2111 non-null   float64\n 4   family_history_with_overweight  2111 non-null   object \n 5   FAVC                            2111 non-null   object \n 6   FCVC                            2111 non-null   float64\n 7   NCP                             2111 non-null   float64\n 8   CAEC                            2111 non-null   object \n 9   SMOKE                           2111 non-null   object \n 10  CH2O                            2111 non-null   float64\n 11  SCC                             2111 non-null   object \n 12  FAF                             2111 non-null   float64\n 13  TUE                             2111 non-null   float64\n 14  CALC                            2111 non-null   object \n 15  MTRANS                          2111 non-null   object \n 16  NObeyesdad                      2111 non-null   object \ndtypes: float64(8), object(9)\nmemory usage: 280.5+ KB\n\n\n\ndata.columns\n\nIndex(['Gender', 'Age', 'Height', 'Weight', 'family_history_with_overweight',\n       'FAVC', 'FCVC', 'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE',\n       'CALC', 'MTRANS', 'NObeyesdad'],\n      dtype='object')"
  },
  {
    "objectID": "OBESITAS.html#eksplorasi-data-grafikan-fitur",
    "href": "OBESITAS.html#eksplorasi-data-grafikan-fitur",
    "title": "1  ",
    "section": "1.5 3. Eksplorasi Data (Grafikan Fitur)",
    "text": "1.5 3. Eksplorasi Data (Grafikan Fitur)\nEksplorasi data, dalam konteks analisis data, adalah proses untuk menggali, memahami, dan memvisualisasikan data dalam rangka memahami pola, tren, dan wawasan yang mungkin terkandung dalam data tersebut. Grafikan fitur (feature visualization) adalah bagian penting dari eksplorasi data yang melibatkan pembuatan grafik atau visualisasi untuk mewakili fitur atau variabel dalam dataset. Pada proses ini bisa dilihat di bawah dengan menggunakan code.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Membuat gambar dengan sembilan subplot dalam tiga baris\nplt.figure(figsize=(18, 9))  # Atur ukuran gambar\n\n# Subplot pertama\nplt.subplot(3, 3, 1)  # (baris, kolom, indeks)\nsns.countplot(x=\"Gender\", data=data)\nplt.xlabel(\"Gender\")\nplt.ylabel(\"Jumlah\")\nplt.title(\"Barplot Gender\")\n\n# Subplot kedua\nplt.subplot(3, 3, 2)  # (baris, kolom, indeks)\nsns.countplot(x=\"family_history_with_overweight\", data=data)\nplt.xlabel(\"Sejarah Overweight dalam Keluarga\")\nplt.ylabel(\"Jumlah\")\nplt.title(\"Barplot Sejarah Overweight dalam Keluarga\")\n\n# Subplot ketiga\nplt.subplot(3, 3, 3)  # (baris, kolom, indeks)\nsns.countplot(x=\"FAVC\", data=data)\nplt.xlabel(\"Sering atau Tidak\")\nplt.ylabel(\"Jumlah\")\nplt.title(\"Barplot Konsumsi Kalori Harian\")\n\n# Subplot keempat\nplt.subplot(3, 3, 4)  # (baris, kolom, indeks)\nsns.countplot(x=\"CAEC\", data=data)\nplt.xlabel(\"Tingkat Seringnya\")\nplt.ylabel(\"Jumlah\")\nplt.title(\"Barplot Konsumsi Makanan setelah Waktu Makan\")\n\n# Subplot kelima\nplt.subplot(3, 3, 5)  # (baris, kolom, indeks)\nsns.countplot(x=\"SMOKE\", data=data)\nplt.xlabel(\"Ya Tidak\")\nplt.ylabel(\"Jumlah\")\nplt.title(\"Barplot Perokok\")\n\n# Subplot keenam\nplt.subplot(3, 3, 6)  # (baris, kolom, indeks)\nsns.countplot(x=\"SCC\", data=data)\nplt.xlabel(\"Ya Tidak\")\nplt.ylabel(\"Jumlah\")\nplt.title(\"Barplot Monitoring Kalori\")\n\n# Subplot ketujuh\nplt.subplot(3, 3, 7)  # (baris, kolom, indeks)\nsns.countplot(x=\"CALC\", data=data)\nplt.xlabel(\"Seringnya Konsumsi\")\nplt.ylabel(\"Jumlah\")\nplt.title(\"Barplot Seringnya Konsumsi Alkohol\")\n\n# Subplot kedelapan\nplt.subplot(3, 3, 8)  # (baris, kolom, indeks)\nsns.countplot(x=\"MTRANS\", data=data)\nplt.xlabel(\"Jenis Transportasi\")\nplt.ylabel(\"Jumlah\")\nplt.title(\"Barplot Jenis Transportasi\")\nplt.xticks(rotation=45)\n\n# Subplot kesembilan\nplt.subplot(3, 3, 9)  # (baris, kolom, indeks)\nsns.countplot(x=\"NObeyesdad\", data=data)\nplt.xlabel(\"Kategori\")\nplt.ylabel(\"Jumlah\")\nplt.title(\"Barplot Overweight\")\nplt.xticks(rotation=45)\n\nplt.tight_layout()  # Agar subplot tidak tumpang tindih\nplt.show()\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Membuat gambar dengan empat subplot-box dalam satu baris\nplt.figure(figsize=(12, 6))  # Atur ukuran gambar\n\n# Subplot pertama\nplt.subplot(2, 4, 1)  # (baris, kolom, indeks)\nsns.boxplot(x=\"Age\", data=data)\nplt.title(\"Boxplot Umur\")\n\n# Subplot kedua\nplt.subplot(2, 4, 2)  # (baris, kolom, indeks)\nsns.boxplot(x=\"Height\", data=data)\nplt.title(\"Boxplot Tinggi Badan\")\n\n# Subplot ketiga\nplt.subplot(2, 4, 3)  # (baris, kolom, indeks)\nsns.boxplot(x=\"Weight\", data=data)\nplt.title(\"Boxplot Berat Badan\")\n\n# Subplot keempat\nplt.subplot(2, 4, 4)  # (baris, kolom, indeks)\nsns.boxplot(x=\"FCVC\", data=data)\nplt.title(\"Boxplot FCVC\")\n\n# Subplot kelima\nplt.subplot(2, 4, 5)  # (baris, kolom, indeks)\nsns.boxplot(x=\"NCP\", data=data)\nplt.title(\"Boxplot NCP\")\n\n# Subplot keenam\nplt.subplot(2, 4, 6)  # (baris, kolom, indeks)\nsns.boxplot(x=\"CH2O\", data=data)\nplt.title(\"Boxplot CH2O\")\n\n# Subplot ketujuh\nplt.subplot(2, 4, 7)  # (baris, kolom, indeks)\nsns.boxplot(x=\"FAF\", data=data)\nplt.title(\"Boxplot FAF\")\n\n# Subplot kedelapan\nplt.subplot(2, 4, 8)  # (baris, kolom, indeks)\nsns.boxplot(x=\"TUE\", data=data)\nplt.title(\"Boxplot TUE\")\n\nplt.tight_layout()  # Agar subplot tidak tumpang tindih\nplt.show()"
  },
  {
    "objectID": "OBESITAS.html#cek-proporsi-setiap-kelas",
    "href": "OBESITAS.html#cek-proporsi-setiap-kelas",
    "title": "1  ",
    "section": "1.6 4. Cek Proporsi Setiap Kelas",
    "text": "1.6 4. Cek Proporsi Setiap Kelas\nCek proporsi kelas pada dataset merupakan langkah yang penting dalam analisis data, terutama dalam konteks klasifikasi atau pemodelan prediktif. Proporsi kelas merujuk pada seberapa seimbang atau tidak seimbangnya jumlah sampel yang termasuk dalam setiap kelas pada suatu dataset. Sebelumnya, kami telah melakukan pengecekan proporsi kelas pada kolom target, dan berikut adalah hasilnya:\n\nObesity_Type_I: 351\nObesity_Type_III: 324\nObesity_Type_II: 297\nOverweight_Level_I: 290\nOverweight_Level_II: 290\nNormal_Weight: 287\nInsufficient_Weight: 272\n\nTerkait dengan distribusi kelas pada kolom target, data tersebut telah diperoleh melalui pemeriksaan kode analisis data. Hasil ini menunjukkan distribusi kelas pada kolom target, yang akan menjadi dasar bagi analisis lebih lanjut dalam pemodelan atau pengambilan keputusan berdasarkan kelas target.\nDalam konteks jumlah sampel antar kelas, data diatas tampak cukup seimbang. Meskipun ada sedikit variasi dalam jumlah sampel antar kelas, perbedaannya tidak signifikan. Dalam banyak kasus, ketidakseimbangan yang dapat memerlukan balancing data biasanya terlihat ketika perbedaan antara jumlah sampel antar kelas sangat besar.\nDengan jumlah sampel yang relatif seragam seperti dalam data diatas, dapat melanjutkan tanpa melakukan balancing data. Namun, penting untuk tetap memonitor kinerja model pada set validasi atau uji dan mengevaluasi metrik evaluasi yang relevan untuk memastikan bahwa model Anda dapat mengatasi semua kelas dengan baik.\nJadi, berdasarkan data diatas, tampaknya tidak perlu dilakukan balancing data karena data sudah relatif seimbang dalam konteks jumlah sampel antar kelas. Tetap awasi kinerja model Anda dan lakukan penyesuaian jika diperlukan berdasarkan evaluasi kinerja aktual pada dataset yang lebih besar atau dataset uji.\n\ndata['NObeyesdad'].value_counts()\n\nObesity_Type_I         351\nObesity_Type_III       324\nObesity_Type_II        297\nOverweight_Level_I     290\nOverweight_Level_II    290\nNormal_Weight          287\nInsufficient_Weight    272\nName: NObeyesdad, dtype: int64"
  },
  {
    "objectID": "OBESITAS.html#mengidentifikasi-missing-value",
    "href": "OBESITAS.html#mengidentifikasi-missing-value",
    "title": "1  ",
    "section": "1.7 5. Mengidentifikasi Missing Value",
    "text": "1.7 5. Mengidentifikasi Missing Value\nFitur-fitur dalam dataset ini telah diperiksa dengan cermat menggunakan kode analisis data, dan hasilnya menunjukkan bahwa tidak ada missing value dalam fitur-fitur tersebut. Selain itu, informasi yang terdapat di website UCI yang menyediakan dataset ini juga mengkonfirmasi bahwa fitur-fitur dalam dataset ini tidak memiliki missing value. Oleh karena itu, kita dapat dengan yakin menganggap bahwa dataset ini telah bersih dari missing value, yang memudahkan analisis data yang lebih lanjut.\nPenjelasan code yang dipakai\n 1. Mengecek apakah ada nilai NaN dalam DataFrame:\n\ndata.isna(): Menghasilkan DataFrame yang memiliki nilai True di setiap sel yang berisi NaN dan False untuk sel yang tidak berisi NaN.\n.sum(): Menghitung jumlah nilai True (1) di setiap kolom. Dengan demikian, kita mendapatkan jumlah nilai NaN dalam setiap kolom.\nHasil cetakan ini akan memberikan jumlah nilai NaN untuk setiap kolom dalam DataFrame.\n\n 2. Menampilkan data yang memiliki nilai NaN:\n\ndata.isna().any(axis=1): Menghasilkan Series boolean yang berisi True jika ada setidaknya satu nilai NaN dalam baris tertentu.\ndata[data.isna().any(axis=1)]: Memilih baris-baris yang memiliki setidaknya satu nilai NaN menggunakan boolean indexing.\nHasil cetakan ini akan menampilkan subset dari DataFrame yang hanya berisi baris-baris yang memiliki setidaknya satu nilai NaN.\n\nDengan cara ini, Anda dapat dengan cepat mengidentifikasi kolom-kolom dan baris-baris tertentu yang mengandung nilai NaN dalam DataFrame Anda. Fitur-fitur dalam dataset ini telah diperiksa dengan cermat menggunakan kode analisis data, dan hasilnya menunjukkan bahwa tidak ada missing value dalam fitur-fitur tersebut. Selain itu, informasi yang terdapat di website UCI yang menyediakan dataset ini juga mengkonfirmasi bahwa fitur-fitur dalam dataset ini tidak memiliki missing value.\n\n# Mengecek apakah ada nilai NaN dalam DataFrame\nprint(data.isna().sum())\n\n# Menampilkan data yang memiliki nilai NaN\nnan_data = data[data.isna().any(axis=1)]\nprint(\"Data dengan nilai NaN:\")\nprint(nan_data)\n\nGender                            0\nAge                               0\nHeight                            0\nWeight                            0\nfamily_history_with_overweight    0\nFAVC                              0\nFCVC                              0\nNCP                               0\nCAEC                              0\nSMOKE                             0\nCH2O                              0\nSCC                               0\nFAF                               0\nTUE                               0\nCALC                              0\nMTRANS                            0\nNObeyesdad                        0\ndtype: int64\nData dengan nilai NaN:\nEmpty DataFrame\nColumns: [Gender, Age, Height, Weight, family_history_with_overweight, FAVC, FCVC, NCP, CAEC, SMOKE, CH2O, SCC, FAF, TUE, CALC, MTRANS, NObeyesdad]\nIndex: []"
  },
  {
    "objectID": "OBESITAS.html#mengidentifikasi-data-outlier",
    "href": "OBESITAS.html#mengidentifikasi-data-outlier",
    "title": "1  ",
    "section": "1.8 6. Mengidentifikasi Data Outlier",
    "text": "1.8 6. Mengidentifikasi Data Outlier\nData outlier (pencilan) merujuk pada nilai atau observasi yang secara signifikan berbeda dari nilai-nilai lain dalam sebuah dataset. Outlier dapat muncul sebagai nilai yang jauh di atas atau di bawah nilai-nilai yang lain, dan mereka dapat memiliki dampak signifikan terhadap analisis statistik dan model prediktif. Identifikasi dan penanganan outlier adalah langkah penting dalam analisis data.\nDampak Data Outlier: - Pengaruh Terhadap Statistik Deskriptif: Outlier dapat mempengaruhi nilai rata-rata (mean) dan menggeser distribusi data, sehingga menghasilkan estimasi yang bias. - Ketidakakuratan Model: Outlier dapat memengaruhi kinerja model prediktif dengan memberikan bobot yang tidak proporsional pada sampel-sampel tertentu. - Risiko Kesalahan Interpretasi: Outlier dapat menyebabkan kesalahan interpretasi dalam analisis data dan menghasilkan kesimpulan yang tidak benar. - Pengaruh Pada Keputusan Bisnis: Outlier yang tidak dikenali atau diabaikan dapat menyebabkan pengambilan keputusan bisnis yang tidak akurat.\nFitur-fitur dalam dataset ini telah diperiksa dengan cermat menggunakan kode analisis data, dan hasilnya menunjukkan bahwa ada data outlier dalam dataset dalam fitur-fitur tersebut. Oleh karena itu data outlier yang dideteksi akan dihapus dari dataset yang akan dilakukan di proses Preprocessing.\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\n# Fungsi untuk mendeteksi outlier menggunakan z-score\ndef has_outliers(df, threshold=3):\n    for kolom in df.columns:\n        # Ambil semua nilai dalam kolom\n        kolom_values = df[kolom]\n\n        # Hanya ambil kolom yang memiliki lebih dari satu nilai (tidak kosong)\n        if len(kolom_values.dropna()) &gt; 1:\n            try:\n                # Coba konversi ke tipe numerik\n                kolom_numerik = pd.to_numeric(kolom_values, errors='coerce')\n                z_scores = np.abs(stats.zscore(kolom_numerik))\n                if np.any(z_scores &gt; threshold):\n                    return True\n            except ValueError:\n                # Tangkap kesalahan jika tidak dapat diubah menjadi numerik\n                pass\n    return False\n\n# Mendeteksi outlier\nif has_outliers(data):\n    print(\"Dataset ini memiliki outlier.\")\nelse:\n    print(\"Dataset ini tidak memiliki outlier.\")\n\nDataset ini memiliki outlier.\n\n\n#\n\n\n\n\nPreprocessing Dataset - - -\n\n\n\n\nPreprocessing data adalah serangkaian langkah yang dilakukan untuk membersihkan, mengubah, dan menyesuaikan data mentah menjadi bentuk yang lebih sesuai untuk analisis atau pemodelan. Tujuan utama dari preprocessing data adalah untuk memastikan bahwa data yang digunakan dalam analisis atau pembangunan model adalah data yang berkualitas tinggi, dapat diandalkan, dan sesuai dengan kebutuhan spesifik proyek atau tugas.\nPreprocessing data memiliki beberapa manfaat yang krusial dalam pengolahan data sebelum dilakukan analisis atau pembangunan model. Berikut adalah beberapa manfaat utama:\nPreprocessing data memiliki beberapa manfaat yang krusial dalam pengolahan data sebelum dilakukan analisis atau pembangunan model. Beberapa manfaat utama dari preprocessing data melibatkan peningkatan kualitas data, penyesuaian data untuk memenuhi persyaratan model, dan meningkatkan performa model. Berikut adalah beberapa manfaat utama:\n 1. Peningkatan Kualitas Data:\n\nPenanganan Nilai yang Hilang: Memastikan bahwa data yang hilang diatasi dengan benar untuk mencegah bias dan informasi yang hilang.\nPembersihan Data: Mengatasi data yang tidak valid atau tidak konsisten yang dapat mengganggu analisis atau pemodelan.\n\n 2. Penyesuaian Data untuk Model:\n\nScaling dan Normalization: Menyesuaikan rentang nilai variabel untuk mencegah dominasi oleh variabel dengan rentang nilai yang besar.\nPengkodean Data Kategorikal: Mengubah variabel kategorikal menjadi bentuk yang dapat diolah oleh algoritma atau model.\nHandling Imbalanced Data: Menangani ketidakseimbangan dalam jumlah sampel antar kelas untuk mencegah bias terhadap kelas mayoritas.\n\n 3. Peningkatan Performa Model:\n\nFeature Engineering: Menciptakan fitur baru atau mengubah fitur yang ada untuk meningkatkan pemahaman model terhadap data.\nHandling Multicollinearity: Mengatasi masalah multikolinearitas yang dapat mempengaruhi interpretasi dan kestabilan model.\nOptimasi Pemisahan Data: Memisahkan data menjadi subset pelatihan dan pengujian untuk menguji model pada data yang tidak terlihat selama pelatihan.\n\n 4. Penghematan Waktu dan Sumber Daya:\n\nPengurangan Kompleksitas: Mengurangi kompleksitas data dengan menghilangkan fitur yang tidak relevan atau menggabungkan fitur yang serupa, sehingga menghemat waktu dan sumber daya komputasi.\n\n 5. Peningkatan Interpretabilitas:\n\nPengkodean Data: Menggunakan pengkodean yang sesuai untuk variabel kategorikal dapat meningkatkan interpretabilitas model.\nFeature Engineering: Menciptakan fitur yang lebih informatif dapat meningkatkan pemahaman tentang faktor-faktor yang mempengaruhi output model.\n\nOleh karena itu, dari analisis data yang sudah kita lakukan, kita mendapatkan informasi seperti permasalahan pada data yang perlu dilakukannya Preprocessing Data, seperti :\n\nTerdapat fitur dengan type object yang harus dirubah menjadi type numerik\nTerdapat data outlier\nNilai - nilai dari fitur tidak serupa\nFitur - fitur yang dianggap tidak penting\n\nUntuk menyelesaikan masalah diatas, maka akan dilakukan beberapa preprocesing, diantaranya :\n1. Encoding 2. Proses Eksekusi Data Outlier 3. Normalisasi 4. Feature Selection 5. Split Dataset"
  },
  {
    "objectID": "OBESITAS.html#encoding",
    "href": "OBESITAS.html#encoding",
    "title": "1  ",
    "section": "1.9 1. Encoding",
    "text": "1.9 1. Encoding\nPreprocessing encoding adalah proses transformasi data mentah atau kategori menjadi format yang dapat dipahami oleh algoritma pembelajaran mesin atau model statistik.\nPada proses preprocessing Encoding menggunakan teknik Label Encoding, dimana Ini digunakan untuk mengganti nilai-nilai dalam variabel kategori dengan bilangan bulat. Setiap kategori diberi label numerik unik. Ini berguna ketika variabel kategori memiliki urutan atau tingkatan yang dapat diurutkan. Contohnya, mengubah kategori “rendah,” “sedang,” “tinggi” menjadi 1, 2, 3.\nContoh :\nMisalkan kita memiliki suatu variabel kategorikal X dengan k nilai unik (kategori). Proses Label Encoding akan mengassign sebuah nilai numerik unik untuk setiap kategori. Rumus Label Encoding dapat dinyatakan sebagai berikut: - Identifikasi nilai unik dalam variabel kategorikal\n\\(X:{xi, x2, ..., xk}\\)\n\nAssign nilai numerik setiap \\(xi\\) menggunakan suatu fungsi atau pemetaan, misalnya :\n\n\\(f(xi) = i\\)\ndimana \\(i\\) adalah urutan dari nilai unik tersebut.\nPada dataset ini fitur yang akan diimplementasikan pada preprocessing encoding ini, sebagai berikut :\n- Gender - Family History With Overweight - FAVC - CAEC - Smoke - SCC - CALC - MTRANS - Nobeyesdad\n\ndata.loc[data[\"Gender\"] == \"Male\", \"Gender\"] = 1\ndata.loc[data[\"Gender\"] == \"Female\", \"Gender\"] = 0\ndata['Gender'] = data.Gender.astype(int)\n\ndata.loc[data[\"family_history_with_overweight\"] == \"yes\", \"family_history_with_overweight\"] = 1\ndata.loc[data[\"family_history_with_overweight\"] == \"no\", \"family_history_with_overweight\"] = 0\ndata['family_history_with_overweight'] = data.family_history_with_overweight.astype(int)\n\ndata.loc[data[\"FAVC\"] == \"yes\", \"FAVC\"] = 1\ndata.loc[data[\"FAVC\"] == \"no\", \"FAVC\"] = 0\ndata['FAVC'] = data.FAVC.astype(int)\n\ndata.loc[data[\"CAEC\"] == \"Always\", \"CAEC\"] = 3\ndata.loc[data[\"CAEC\"] == \"Frequently\", \"CAEC\"] = 2\ndata.loc[data[\"CAEC\"] == \"Sometimes\", \"CAEC\"] = 1\ndata.loc[data[\"CAEC\"] == \"no\", \"CAEC\"] = 0\ndata['CAEC'] = data.CAEC.astype(int)\n\ndata.loc[data[\"SMOKE\"] == \"yes\", \"SMOKE\"] = 1\ndata.loc[data[\"SMOKE\"] == \"no\", \"SMOKE\"] = 0\ndata['SMOKE'] = data.SMOKE.astype(int)\n\ndata.loc[data[\"SCC\"] == \"yes\", \"SCC\"] = 1\ndata.loc[data[\"SCC\"] == \"no\", \"SCC\"] = 0\ndata['SCC'] = data.SCC.astype(int)\n\ndata.loc[data[\"CALC\"] == \"Always\", \"CALC\"] = 3\ndata.loc[data[\"CALC\"] == \"Frequently\", \"CALC\"] = 2\ndata.loc[data[\"CALC\"] == \"Sometimes\", \"CALC\"] = 1\ndata.loc[data[\"CALC\"] == \"no\", \"CALC\"] = 0\ndata['CALC'] = data.CALC.astype(int)\n\ndata.loc[data[\"MTRANS\"] == \"Automobile\", \"MTRANS\"] = 4\ndata.loc[data[\"MTRANS\"] == \"Motorbike\", \"MTRANS\"] = 3\ndata.loc[data[\"MTRANS\"] == \"Bike\", \"MTRANS\"] = 2\ndata.loc[data[\"MTRANS\"] == \"Public_Transportation\", \"MTRANS\"] = 1\ndata.loc[data[\"MTRANS\"] == \"Walking\", \"MTRANS\"] = 0\ndata['MTRANS'] = data.MTRANS.astype(int)\n\ndata.loc[data[\"NObeyesdad\"] == \"Obesity_Type_I\", \"NObeyesdad\"] = 6\ndata.loc[data[\"NObeyesdad\"] == \"Obesity_Type_III\", \"NObeyesdad\"] = 5\ndata.loc[data[\"NObeyesdad\"] == \"Obesity_Type_II\", \"NObeyesdad\"] = 4\ndata.loc[data[\"NObeyesdad\"] == \"Overweight_Level_I\", \"NObeyesdad\"] = 3\ndata.loc[data[\"NObeyesdad\"] == \"Overweight_Level_II\", \"NObeyesdad\"] = 2\ndata.loc[data[\"NObeyesdad\"] == \"Normal_Weight\", \"NObeyesdad\"] = 1\ndata.loc[data[\"NObeyesdad\"] == \"Insufficient_Weight\", \"NObeyesdad\"] = 0\ndata['NObeyesdad'] = data.NObeyesdad.astype(int)"
  },
  {
    "objectID": "OBESITAS.html#eksekusi-data-outlier",
    "href": "OBESITAS.html#eksekusi-data-outlier",
    "title": "1  ",
    "section": "1.10 2. Eksekusi Data Outlier",
    "text": "1.10 2. Eksekusi Data Outlier\nOutlier adalah titik data yang signifikan atau ekstrem yang berbeda secara signifikan dari sebagian besar data dalam suatu himpunan. Dalam konteks statistik dan analisis data, outlier dapat memberikan dampak yang signifikan pada hasil analisis karena mereka mewakili nilai-nilai yang jauh dari kebanyakan data. Oleh karena itu, mengidentifikasi dan menangani outlier menjadi langkah penting dalam menyediakan data yang baik untuk analisis atau pembangunan model. Cek data outlier ini memanfaatkan z-score untuk mendeteksi outlier dan kemudian menghapusnya dari DataFrame. Berikut adalah langkah dalam cek dan penanganan data outlier :\n1. Funsgi ‘detect_outliers’ : - Fungsi ini menerima DataFrame dan nilai ambang (threshold) sebagai parameter. - Menghitung z-score untuk setiap elemen dalam DataFrame menggunakan fungsi ‘stats.zscore’. - Mengidentifikasi outlier berdasarkan threshold yang diberikan. - Mengembalikan daftar pasangan indeks baris dan kolom yang mengandung outlier.\n2. Perhitungan Z-Score: Z-scores dihitung untuk setiap elemen dalam numerical_data menggunakan rumus:\n\\(Z = \\frac{X - \\mu}{\\sigma}\\)\n\nμ adalah rata-rata\n\nσ adalah deviasi standar.\n\n3. Mendeteksi dan Mengatasi Outlier :\n\nMemanggil fungsi detect_outliers untuk mendapatkan daftar outlier.\nJika outlier ditemukan, mencetak informasi tentang outlier yang terdeteksi dan total outlier.\nMenghapus outlier dari DataFrame menggunakan nilai indeks yang ditemukan.\nMencetak jumlah baris setelah menghapus outlier jika ada outlier, dan mencetak pesan bahwa tidak ada outlier jika tidak ada.\n\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\n# Fungsi untuk mendeteksi outlier menggunakan z-score\ndef detect_outliers(df, threshold=3):\n    z_scores = np.abs(stats.zscore(data))\n    row_outliers, col_outliers = np.where(z_scores &gt; threshold)\n    return list(zip(row_outliers, col_outliers))\n\n# Mendeteksi outlier\noutliers = detect_outliers(data)\n\nif outliers:\n    print(\"Outlier(s) terdeteksi pada baris dan kolom berikut:\")\n\n    # Menghitung jumlah outlier\n    total_outliers = len(outliers)\n    print(f\"Total outlier: {total_outliers}\")\n    # Menghapus outlier\n    data_no_outliers = data.copy()\n    for row, col in outliers:\n        if row in data_no_outliers.index:\n            data_no_outliers = data_no_outliers.drop(index=row)\n\n    # Menghitung jumlah baris tanpa outlier\n    rows_without_outliers = len(data_no_outliers)\n    print(f\"Jumlah baris tanpa outlier: {rows_without_outliers}\")\n\nelse:\n    print(\"Tidak ada outlier dalam data.\")\n\nOutlier(s) terdeteksi pada baris dan kolom berikut:\nTotal outlier: 219\nJumlah baris tanpa outlier: 1914\n\n\n\n# Menghitung jumlah target pada data tanpa outlier\ntarget_no_outliers = data_no_outliers['NObeyesdad'].value_counts()\n\nprint(\"Jumlah target pada data tanpa outlier:\")\nprint(target_no_outliers)\n\nJumlah target pada data tanpa outlier:\n6    330\n5    322\n4    280\n2    267\n3    248\n0    247\n1    220\nName: NObeyesdad, dtype: int64"
  },
  {
    "objectID": "OBESITAS.html#normalisasi",
    "href": "OBESITAS.html#normalisasi",
    "title": "1  ",
    "section": "1.11 3. Normalisasi",
    "text": "1.11 3. Normalisasi\nNormalisasi Min-Max adalah salah satu teknik preprocessing data yang digunakan untuk mengubah skala data numerik ke dalam rentang tertentu, biasanya dari 0 hingga 1. Tujuan utama dari normalisasi Min-Max adalah untuk menjaga skala data sehingga nilai-nilai dari berbagai fitur atau variabel memiliki rentang yang serupa, sehingga model pembelajaran mesin dapat bekerja lebih efisien dan tidak terpengaruh oleh perbedaan skala.\nRumus Normalisasi MinMax:\n\\[X_{\\text{normalized}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\\]\nKeterangan :\n- X normalized​: Nilai fitur yang telah dinormalisasi - X: Nilai asli fitur - Xmin​: Nilai minimum fitur - Xmax​: Nilai maksimum fitur\nContoh Perhitungan MinMax :\n\n\n\nUmur\nUmur Normalisasi\n\n\n\n\n40\n0\n\n\n50\n0.33\n\n\n60\n0.66\n\n\n70\n1\n\n\n55\n0.5\n\n\n\nKita ambil satu nilai dari umur yaitu 40, sebelumnya diketahui nilai max dan nilai min :\n- Nilai Max = 70 - Nilai Min = 40\nlalu kita masukkan ke dalam rumus menjadi :\n\\[X_{\\text{normalized}} = \\frac{(40 - 40)}{({70} - {40})} = 0 \\]\nBerikut adalah langkah-langkah umum dalam normalisasi Min-Max:\n\nPilih Rentang Normalisasi: Biasanya, rentang normalisasi adalah antara 0 hingga 1, tetapi Anda juga dapat memilih rentang lain, tergantung pada kebutuhan Anda.\nHitung Nilai Minimum dan Maksimum: Temukan nilai minimum (Min) dan maksimum (Max) dari setiap fitur atau variabel dalam dataset Anda.\nGunakan Rumus Normalisasi Min-Max: Untuk setiap nilai dalam fitur, gunakan rumus berikut untuk menghitung nilai yang telah dinormalisasi (X_normalized):\n\n\\[X_{\\text{normalized}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\\]\n\nTerapkan Normalisasi: Terapkan rumus normalisasi Min-Max pada semua nilai dalam setiap fitur, sehingga semua fitur memiliki nilai yang telah dinormalisasi dalam rentang yang dipilih\n\n\nimport pickle\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Inisialisasi Min-Max Scaler\nscaler = MinMaxScaler()\n\n# Fit dan transformasi pada fitur-fitur numerik yang ingin dinormalisasi\nnumerical_features = ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']\ndata_no_outliers[numerical_features] = scaler.fit_transform(data_no_outliers[numerical_features])\n\n# Simpan scaler ke dalam model pickle\nwith open('scaler.pkl', 'wb') as file:\n    pickle.dump(scaler, file)\n\n\n# Simpan dataset setelah kedua preprocessing ke dalam file CSV\ndata_no_outliers.to_csv('new_preprocessing_obesitas.csv', index=False)\n\n\ndata = pd.read_csv('new_preprocessing_obesitas.csv')\ndata\n\n\n\n\n\n\n\n\nGender\nAge\nHeight\nWeight\nfamily_history_with_overweight\nFAVC\nFCVC\nNCP\nCAEC\nSMOKE\nCH2O\nSCC\nFAF\nTUE\nCALC\nMTRANS\nNObeyesdad\n\n\n\n\n0\n0\n0.239411\n0.280007\n0.198323\n1\n0\n0.5\n0.666667\n1\n0\n0.500000\n0\n0.000000\n0.500000\n0\n1\n1\n\n\n1\n1\n0.307814\n0.644393\n0.301450\n1\n0\n0.5\n0.666667\n1\n0\n0.500000\n0\n0.666667\n0.500000\n2\n1\n1\n\n\n2\n1\n0.444621\n0.644393\n0.380779\n0\n0\n1.0\n0.666667\n1\n0\n0.500000\n0\n0.666667\n0.000000\n2\n0\n3\n\n\n3\n1\n0.273613\n0.603906\n0.402991\n0\n0\n0.5\n0.000000\n1\n0\n0.500000\n0\n0.000000\n0.000000\n1\n1\n2\n\n\n4\n1\n0.513024\n0.280007\n0.111061\n0\n1\n0.5\n0.666667\n1\n0\n0.500000\n0\n0.000000\n0.000000\n1\n4\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1909\n0\n0.238619\n0.463678\n0.733068\n1\n1\n1.0\n0.666667\n1\n0\n0.364069\n0\n0.558756\n0.453124\n1\n1\n5\n\n\n1910\n0\n0.273029\n0.540308\n0.751587\n1\n1\n1.0\n0.666667\n1\n0\n0.502565\n0\n0.447130\n0.299635\n1\n1\n5\n\n\n1911\n0\n0.291536\n0.547640\n0.751161\n1\n1\n1.0\n0.666667\n1\n0\n0.527097\n0\n0.471403\n0.323144\n1\n1\n5\n\n\n1912\n0\n0.354395\n0.521818\n0.748443\n1\n1\n1.0\n0.666667\n1\n0\n0.926169\n0\n0.379702\n0.293017\n1\n1\n5\n\n\n1913\n0\n0.330548\n0.520575\n0.749442\n1\n1\n1.0\n0.666667\n1\n0\n0.931756\n0\n0.342151\n0.357069\n1\n1\n5\n\n\n\n\n1914 rows × 17 columns"
  },
  {
    "objectID": "OBESITAS.html#feature-selection",
    "href": "OBESITAS.html#feature-selection",
    "title": "1  ",
    "section": "1.12 4. Feature Selection",
    "text": "1.12 4. Feature Selection\nFeature Selection (pemilihan fitur) adalah proses memilih subset fitur yang paling relevan dan informatif dari suatu dataset untuk digunakan dalam pembangunan model atau analisis data. Tujuan utama dari feature selection adalah untuk meningkatkan kinerja model dengan mengurangi kompleksitas dan meningkatkan interpretabilitas, sambil mempertahankan atau meningkatkan keakuratan prediksi. Pada proses seleksi fitur ini menggunakan fungsi Mutual Information.\nMutual Information (Informasi Timbal Balik) adalah suatu metrik yang digunakan untuk mengukur seberapa banyak pengetahuan tentang suatu variabel dapat memberikan wawasan tentang variabel lainnya. Dalam konteks feature selection atau pemilihan fitur, mutual information digunakan untuk mengukur seberapa informatif suatu fitur terhadap variabel target atau variabel kelas.\nRumus Mutual Information : \n\\[I(X;Y) = \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log\\left(\\frac{p(x, y)}{p(x)p(y)}\\right)\\]\ndimana :\n- \\(p(x,y)\\) adalah probabilitas bersama dari \\(X\\) dan \\(Y\\). - \\(p(x)\\) dan \\(p(y)\\) adalah probabilitas marginal dari \\(X\\) dan \\(Y\\) masing - masing.\nBerikut langkah - langkah yang digunakan pada code :\n1. Hitung Mutual Information : - Menghitung nilai mutual information antara setiap fitur dalam DataFrame X dan target variabel kelas Y. - Hasilnya adalah sebuah array yang berisi nilai mutual information untuk setiap fitur.\n2. Buat series pandas untuk visualisasi : - Membuat objek Series Pandas (‘feat_importances’) untuk menyimpan nilai mutual information bersama dengan nama fitur yang sesuai. - Menggunakan ‘data.columns[0:len(data.columns)-1]’ untuk mengambil nama kolom dari DataFrame ‘data’ (kecuali kolom terakhir yang merupakan target variabel kelas).\n3. Visualisasi Bar Plot : - Membuat diagram batang horizontal (barh) untuk memvisualisasikan nilai mutual information. - Menggunakan warna ‘teal’ untuk plot. - Menampilkan plot menggunakan plt.show.\nJadi, hasilnya adalah diagram batang horizontal yang menunjukkan seberapa informatif setiap fitur dalam kaitannya dengan variabel kelas. Fitur dengan nilai mutual information yang lebih tinggi dianggap lebih informatif terkait dengan variabel kelas. Plot ini membantu dalam pemahaman relatif antara setiap fitur dan target variabel kelas.\nKonsep cara kerja dari Mutual Information :\nSeleksi fitur menggunakan metode informasi bersama (mutual information) adalah salah satu pendekatan yang digunakan dalam pemrosesan dan analisis data untuk menentukan fitur-fitur mana yang memiliki hubungan yang paling kuat dengan variabel target. Mutual information adalah ukuran statistik yang mengukur seberapa banyak informasi dari satu variabel yang dapat memberikan informasi tentang variabel lain. Dalam konteks seleksi fitur, kita mencari fitur yang memberikan informasi maksimal tentang variabel target. 1. Mutual Information: Mutual information mengukur sejauh mana pengetahuan tentang nilai satu variabel dapat memberikan informasi tentang nilai variabel lain. Nilai mutual information tinggi menunjukkan bahwa dua variabel memiliki hubungan yang kuat. 2. Pemilihan Fitur: Tujuan seleksi fitur menggunakan mutual information adalah memilih fitur-fitur yang memiliki hubungan yang signifikan dengan variabel target. Fitur-fitur ini dianggap memberikan kontribusi maksimal dalam memprediksi variabel target. 3. Perbandingan Antar Fitur: Setiap fitur dibandingkan dengan variabel target, dan nilai mutual information dihitung. Fitur-fitur dengan nilai mutual information tertinggi dipilih sebagai fitur yang paling informatif. 4. Implementasi: Pada praktiknya, seleksi fitur dengan mutual information dapat diimplementasikan menggunakan algoritma atau fungsi yang telah disediakan oleh berbagai pustaka atau modul dalam berbagai bahasa pemrograman. Beberapa pustaka umum termasuk scikit-learn untuk Python dan scikit-feature untuk MATLAB. 5. Langkah-langkah Seleksi Fitur: Identifikasi dan pemisahan variabel input (fitur) dan variabel output (target). Hitung nilai mutual information antara setiap fitur dan variabel target. Pilih fitur-fitur dengan nilai mutual information tertinggi sebagai fitur-fitur yang akan digunakan dalam model atau analisis berikutnya. 6. Keuntungan: Reduksi dimensi dataset. Fokus pada fitur-fitur yang paling informatif. Meningkatkan interpretabilitas model. 7. Catatan Penting: Seleksi fitur dengan mutual information harus dilakukan dengan hati-hati untuk menghindari overfitting. Evaluasi performa model setelah seleksi fitur adalah langkah penting.\n\ndata = data.copy()\nY = data['NObeyesdad']\nX = data.drop(columns = ['NObeyesdad'])\n\n\nfrom sklearn.feature_selection import mutual_info_classif\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimportances = mutual_info_classif(X,Y)\nfeat_importances = pd.Series(importances, data.columns[0:len(data.columns)-1])\nfeat_importances.plot(kind='barh', color = 'teal')\nplt.show\n\n&lt;function matplotlib.pyplot.show(close=None, block=None)&gt;\n\n\n\n\n\n\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\n# Pisahkan fitur dan target\nX = data.drop(columns=['NObeyesdad'])  # Fitur\nY = data['NObeyesdad']  # Target\n\n# Inisialisasi SelectKBest dengan mutual_info_classif sebagai skor fungsi\n# Pilih k fitur terbaik\nk_best = SelectKBest(mutual_info_classif, k='all')\n\n# Terapkan seleksi pada data\nX_new = k_best.fit_transform(X, Y)\n\n# Dapatkan peringkat fitur\nfeature_ranks = k_best.scores_\n\n# Gabungkan nama fitur dan peringkatnya\nfeature_rank_df = pd.DataFrame({'Feature': X.columns, 'Rank': feature_ranks})\n\n# Urutkan berdasarkan peringkat\nfeature_rank_df = feature_rank_df.sort_values(by='Rank', ascending=False)\n\n# Tampilkan hasil\nprint(feature_rank_df)\n\n                           Feature      Rank\n3                           Weight  1.280959\n1                              Age  0.600186\n2                           Height  0.425632\n6                             FCVC  0.405720\n10                            CH2O  0.303303\n12                             FAF  0.283500\n13                             TUE  0.274901\n7                              NCP  0.270292\n0                           Gender  0.251399\n4   family_history_with_overweight  0.161614\n8                             CAEC  0.150322\n14                            CALC  0.107645\n15                          MTRANS  0.067694\n5                             FAVC  0.063384\n11                             SCC  0.002825\n9                            SMOKE  0.000000"
  },
  {
    "objectID": "OBESITAS.html#split-dataset",
    "href": "OBESITAS.html#split-dataset",
    "title": "1  ",
    "section": "1.13 5. Split Dataset",
    "text": "1.13 5. Split Dataset\nPembagian dataset (split dataset) merujuk pada proses memisahkan dataset menjadi dua atau lebih bagian untuk digunakan dalam pelatihan dan pengujian model. Pembagian ini diperlukan untuk menguji kinerja model pada data yang tidak digunakan selama pelatihan, sehingga kita dapat mengukur sejauh mana model tersebut dapat menggeneralisasi pola dari data yang belum pernah dilihat sebelumnya. Dengan melakukan pembagian dataset, kita dapat menghindari overfitting model pada data pelatihan dan memberikan perkiraan yang lebih baik tentang seberapa baik model akan berkinerja pada data yang belum pernah dilihat sebelumnya.\nPenjelasan code yang digunakan 1. Memisahkan Fitur dan Target: Dataset asli (data) dibagi menjadi dua bagian: fitur (X) dan target (Y). Fitur adalah kolom data yang akan digunakan sebagai input untuk model pembelajaran mesin, sedangkan target adalah kolom yang akan diprediksi oleh model. Dalam kasus ini, kolom ‘NObeyesdad’ adalah target, sementara kolom lainnya dihilangkan (drop) dari fitur.\n\nMemisahkan Data Latihan dan Data Uji: Fungsi train_test_split digunakan untuk membagi dataset menjadi data latihan (X_train dan Y_train) dan data uji (X_test dan Y_test). Parameter test_size = 0.2 menentukan bahwa 20% dari data akan menjadi data uji, sementara 80% sisanya akan menjadi data latihan. Parameter random_state = 42 digunakan untuk mengatur seed (bilangan acak awal) sehingga pembagian dataset ini dapat direproduksi jika diperlukan.\nMenampilkan Informasi Dataset: Kode selanjutnya mencetak jumlah total data dalam dataset (X.shape[0]), jumlah data latihan (X_train.shape[0]), dan jumlah data uji (X_test.shape[0]). Ini membantu Anda memahami berapa banyak data yang digunakan untuk melatih model dan seberapa banyak data yang akan digunakan untuk menguji model.\n\nOleh karena itu dataset yang akan di drop atau tidak digunakan untuk prediksi atau tidak masuk kedalam proses modeling adalah :\n- Nobyesdad - FAVC - SMOKE - CALC - SCC - MTRANS - family_history_with_overweight\nFitur diatas tidak masuk kedalam modelling karena tidak semua fitur mempunyai nilai fungsi yang tinggi, oleh karena itu harus di lakukan drop dan hanya mengambil fitur yang penting saja, selain itu setelah fitur - fitur di atas di drop hasil akurasi di Modelling menjadi lebih baik.\n\nfrom sklearn.model_selection import train_test_split\n\n# memisahkan kolom fitur dan target\nX = data.drop(columns=['NObeyesdad','FAVC','CAEC','SMOKE','SCC','CALC','MTRANS','family_history_with_overweight','Gender'], axis =1)\nY = data['NObeyesdad']\n\n# membagi dataset menjadi data training dan data testing\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n\n# banyaknya data uji data data testing\nprint(\"Jumlah Data : \", X.shape[0])\nprint(\"Data Latih : \", X_train.shape[0])\nprint(\"Data Uji   : \", X_test.shape[0])\n\n# fitur.shape, fitur_train.shape, fitur_test.shape\n\nJumlah Data :  1914\nData Latih :  1531\nData Uji   :  383\n\n\n#\n\n\n\n\nModelling - - -\n\n\n\n\nPemodelan (modeling) dalam konteks machine learning adalah proses pembuatan model matematis atau statistik yang dapat memahami pola dalam data dan digunakan untuk membuat prediksi atau keputusan tanpa melibatkan secara eksplisit aturan atau logika yang telah diprogram sebelumnya. Pemodelan merupakan bagian integral dari berbagai aplikasi machine learning, termasuk prediksi, klasifikasi, klastering, dan optimisasi, untuk mencapai pemahaman lebih mendalam tentang data dan membuat keputusan yang lebih baik.\nManfaat dari proses Modelling : - Prediksi dan Estimasi: Memungkinkan kita untuk membuat prediksi atau estimasi pada data yang belum pernah dilihat sebelumnya berdasarkan pola yang telah dipelajari dari data pelatihan. - Pemahaman Data: Membantu dalam pemahaman pola atau hubungan dalam data yang mungkin sulit atau tidak mungkin dipahami dengan cara konvensional. Model dapat mengungkapkan relasi kompleks antara fitur-fitur dan variabel target. - Optimisasi: Digunakan dalam situasi di mana kita ingin mengoptimalkan suatu variabel target atau mencapai hasil terbaik berdasarkan kondisi atau batasan tertentu. - Klasifikasi: Dapat digunakan untuk mengelompokkan data ke dalam kategori atau kelas tertentu, seperti dalam klasifikasi spam atau non-spam, identifikasi gambar, atau diagnosa medis. - Regresi: Berguna dalam memodelkan hubungan antara variabel dependen dan independen, membantu dalam meramalkan atau mengukur nilai variabel dependen berdasarkan nilai-nilai variabel independen. - Klastering: Memungkinkan kita untuk mengelompokkan data ke dalam kelompok atau klaster berdasarkan kesamaan atau pola tertentu, membantu dalam pemahaman struktur data yang kompleks. - Optimasi Proses Bisnis: Dapat digunakan untuk mengoptimalkan proses bisnis dengan mengidentifikasi faktor-faktor kunci yang mempengaruhi hasil bisnis dan memberikan rekomendasi untuk peningkatan. - Pemilihan Fitur: Dapat membantu mengidentifikasi fitur-fitur yang paling relevan dan berkontribusi pada kinerja model. Ini membantu mengurangi kompleksitas model dan meningkatkan interpretabilitas.\nPada Proses Modelling ini akan mendapatkan beberapa informasi, yaitu : 1. Perbandingan beberapa Metode, terdapat 5 metode yang akan dibandingkan nilai akurasi nya, yaitu: - Linier Regression - KNN - SVM - Decission Tree - Random Forest\n\nDapatkan Metode Terbaik\nModelling dengan metode terbaik"
  },
  {
    "objectID": "OBESITAS.html#perbandingan-beberapa-metode",
    "href": "OBESITAS.html#perbandingan-beberapa-metode",
    "title": "1  ",
    "section": "1.14 1. Perbandingan Beberapa Metode",
    "text": "1.14 1. Perbandingan Beberapa Metode\n\n1.14.1 - Linear Regression\nRegresi linear adalah salah satu teknik dalam statistika dan machine learning yang digunakan untuk memodelkan hubungan linier antara satu atau lebih variabel independen (fitur) dengan variabel dependen (target). Tujuan utama dari regresi linear adalah menemukan garis lurus terbaik (linear) yang paling baik mewakili hubungan antara variabel-variabel tersebut.\nModel regresi linear memiliki bentuk umum:\n\\(Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_nX_n + \\epsilon\\)\nDimana: - \\(Y\\) adalah variabel dependen (target) - \\(X1, X2, X3, ...\\) adalah variabel independen (fitur) - \\(\\beta_0, \\beta_1, \\beta_2, ....\\) adalah sebagai koefisien regresi yang mengukur seberapa besar perubahan dalam \\(Y\\) yang diakibatkan oleh perubahan satu unit dalam \\(X1, X2, X3, ...\\) masing - masing - \\(\\epsilon\\) adalah kesalahan acak (residuals) yang merupakan faktor yang tidak dapat dijelaskan oleh model\n\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train,Y_train)\nY_pred = model.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nprint(\"Accuracy Score:\",accuracy_score(Y_test,Y_pred))\n\nAccuracy Score: 0.6866840731070496\n\n\n\n\n1.14.2 - KNN\nK-Nearest Neighbors (KNN) adalah salah satu algoritma pembelajaran mesin yang digunakan untuk tugas-tugas klasifikasi dan regresi. KNN termasuk dalam kategori algoritma berbasis instansi atau lazy learning, yang berarti algoritma tersebut tidak “mempelajari” model dari data pelatihan seperti kebanyakan algoritma pembelajaran mesin, melainkan menyimpan seluruh dataset pelatihan dan melakukan prediksi berdasarkan kedekatan antara data baru dan data pelatihan yang telah ada.\nRumus umum KNN untuk klasifikasi:\n\\(\\hat{y} = \\text{argmax}_{j} \\sum_{i=1}^{K} I(y_i = j)\\)\ndimana: - \\(\\hat{y}\\) adalah prediksi kelas untuk data baru. - \\(y_i\\) adalah kelas dari tetangga ke-i. - \\(I\\) adalah fungsi indikator yang bernilai 1 jika pernyataan di dalamnya benar, dan 0 jika salah. - \\(\\text{argmax}_{j}\\) mengambil kelas yang memiliki jumlah tetangga terbanyak.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom warnings import filterwarnings\nfilterwarnings(action='ignore')\n\nakurasi_terbaik = 0\nk_terbaik = 0\n\nfor k in range(1, 21):\n    model = KNeighborsClassifier(n_neighbors=k)\n    model.fit(X_train, Y_train)\n    hasil_prediksi = model.predict(X_test)\n\n    akurasi = accuracy_score(Y_test, hasil_prediksi)\n    print(f\"k = {k}, Skor Akurasi: {akurasi}\")\n\n    if akurasi &gt; akurasi_terbaik:\n        akurasi_terbaik = akurasi\n        k_terbaik = k\n\nprint(f\"k Terbaik: {k_terbaik} dengan Skor Akurasi: {akurasi_terbaik}\")\n\nk = 1, Skor Akurasi: 0.835509138381201\nk = 2, Skor Akurasi: 0.8302872062663186\nk = 3, Skor Akurasi: 0.8198433420365535\nk = 4, Skor Akurasi: 0.814621409921671\nk = 5, Skor Akurasi: 0.7989556135770235\nk = 6, Skor Akurasi: 0.7911227154046997\nk = 7, Skor Akurasi: 0.7754569190600522\nk = 8, Skor Akurasi: 0.7650130548302873\nk = 9, Skor Akurasi: 0.7728459530026109\nk = 10, Skor Akurasi: 0.7519582245430809\nk = 11, Skor Akurasi: 0.7493472584856397\nk = 12, Skor Akurasi: 0.7441253263707572\nk = 13, Skor Akurasi: 0.741514360313316\nk = 14, Skor Akurasi: 0.7310704960835509\nk = 15, Skor Akurasi: 0.7258485639686684\nk = 16, Skor Akurasi: 0.7154046997389034\nk = 17, Skor Akurasi: 0.720626631853786\nk = 18, Skor Akurasi: 0.7154046997389034\nk = 19, Skor Akurasi: 0.7127937336814621\nk = 20, Skor Akurasi: 0.7101827676240209\nk Terbaik: 1 dengan Skor Akurasi: 0.835509138381201\n\n\n\n\n1.14.3 - SVM\nSupport Vector Machine (SVM) adalah algoritma pembelajaran mesin yang dapat digunakan untuk tugas klasifikasi atau regresi. SVM berfokus pada menemukan hyperplane terbaik yang memisahkan dua kelas dalam ruang fitur. Hyperplane ini dipilih agar memiliki margin terbesar antara dua kelas.\nRumus umum:\n\\(f(x) = \\text{sign}(\\mathbf{w} \\cdot \\mathbf{x} + b)\\)\ndimana: - \\(\\mathbf{w}\\) adalah vektor bobot, - \\(\\mathbf{x}\\) adalah vektor fitur instance, - \\(b\\) adalah bias, - \\(\\text{sign}(\\cdot)\\) adalah fungsi signum yang mengembalikan 1 jika argumen positif, -1 jika negatif.\n\nfrom sklearn.svm import SVC\nmodel = SVC()\nmodel.fit(X_train,Y_train)\npred_y = model.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy Score:\",accuracy_score(Y_test,pred_y))\n\nAccuracy Score: 0.8720626631853786\n\n\n\n\n1.14.4 - Decission Tree\nDecision Tree (Pohon Keputusan) adalah algoritma pembelajaran mesin yang digunakan untuk tugas klasifikasi dan regresi. Decision Tree mengambil keputusan berdasarkan serangkaian aturan yang dibangun dari data pelatihan.\nRumus umum klasifikasi:\n\\(\\hat{y} = f(x)\\)\n\nfrom sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(criterion='entropy',random_state=42)\nmodel.fit(X_train,Y_train)\ny_pred = model.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy Score:\",accuracy_score(Y_test,y_pred))\n\nAccuracy Score: 0.9686684073107049\n\n\n\n\n1.14.5 - Random Forest\nRandom Forest adalah suatu ansambel model pohon keputusan yang digunakan untuk tugas klasifikasi dan regresi. Ansambel ini menggabungkan prediksi dari beberapa pohon keputusan untuk meningkatkan kinerja dan ketahanan terhadap overfitting.\nRumum umum untuk prediksi:\n\nKlasifikasi (Mayoritas Voting)\n\n\\(\\hat{y} = \\text{mode}(\\hat{y}_1, \\hat{y}_2, \\ldots, \\hat{y}_n)\\)\n\nKlasifikasi (Probabilitas Rata-rata)\n\n\\(\\hat{y} = \\frac{1}{n} \\sum_{i=1}^{n} \\hat{y}_i\\)\n\nRegresi (Rata-rata)\n\n\\(\\hat{y} = \\frac{1}{n} \\sum_{i=1}^{n} \\hat{y}_i\\)\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport pickle\n\nbest_accuracy = 0.0\nbest_rf_model = None\nbest_n_estimators = 0\n\nfor n_estimators in range(1, 21):\n    rf_model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n    rf_model.fit(X, Y)\n\n    y_pred = rf_model.predict(X)\n    accuracy = accuracy_score(Y, y_pred)\n\n    print(f\"n_estimators = {n_estimators}, Akurasi: {accuracy:.2%}\")\n\n    if accuracy &gt; best_accuracy:\n        best_accuracy = accuracy\n        best_rf_model = rf_model\n        best_n_estimators = n_estimators\n\nprint(f\"\\nAkurasi terbaik: {best_accuracy:.2%} dengan n_estimators: {best_n_estimators}.\")\n\nn_estimators = 1, Akurasi: 95.09%\nn_estimators = 2, Akurasi: 95.14%\nn_estimators = 3, Akurasi: 98.90%\nn_estimators = 4, Akurasi: 98.80%\nn_estimators = 5, Akurasi: 99.27%\nn_estimators = 6, Akurasi: 99.48%\nn_estimators = 7, Akurasi: 99.58%\nn_estimators = 8, Akurasi: 99.69%\nn_estimators = 9, Akurasi: 99.69%\nn_estimators = 10, Akurasi: 99.74%\nn_estimators = 11, Akurasi: 99.84%\nn_estimators = 12, Akurasi: 99.84%\nn_estimators = 13, Akurasi: 99.79%\nn_estimators = 14, Akurasi: 99.90%\nn_estimators = 15, Akurasi: 99.90%\nn_estimators = 16, Akurasi: 99.95%\nn_estimators = 17, Akurasi: 99.95%\nn_estimators = 18, Akurasi: 99.95%\nn_estimators = 19, Akurasi: 99.90%\nn_estimators = 20, Akurasi: 99.95%\n\nAkurasi terbaik: 99.95% dengan n_estimators: 16."
  },
  {
    "objectID": "OBESITAS.html#dapatkan-metode-terbaik",
    "href": "OBESITAS.html#dapatkan-metode-terbaik",
    "title": "1  ",
    "section": "1.15 2. Dapatkan Metode Terbaik",
    "text": "1.15 2. Dapatkan Metode Terbaik\nDari hasil perbandingan diatas dengan telah melatih dan mengevaluasi beberapa metode klasifikasi yang berbeda untuk memprediksi target yang sesuai dengan dataset kami. Metode yang kami uji meliputi - Logistic Regression - K-Nearest Neighbors (KNN) - Support Vector Machine (SVM) - Decision Tree - Random Forest\nSetiap model telah melalui proses pelatihan menggunakan data latih dan diuji pada dataset uji.\nSetelah evaluasi akurasi masing-masing model, hasil menunjukkan bahwa metode terbaik untuk tugas klasifikasi ini adalah Random Forest. Model ini mencapai akurasi tertinggi dibandingkan dengan model-model lain yang diuji. Oleh karena itu, Random Forest dipilih sebagai model yang optimal untuk memprediksi target pada dataset ini.\nBerikut langkah - langkah yang dilakukan pada code :\n1. Pembagian Data Train dan Test :\n\nMembagi dataset menjadi data latih(X_train, Y_train) dan data uji (X_test, Y_test) menggunakan fungsi train_test_split. Data uji merupakan 20% dari keseluruhan data, dan random_state digunakan untuk memastikan reproducibility.\n\n2. Inisialisasi Model :\n\nMembuat dictionary (models) yang berisi beberapa jenis model klasifikasi seperti Regresi Logistik, KNN (K-Nearest Neighbors), SVM (Support Vector Machine), Decision Tree, dan Random Forest.\n\n3. Pelatihan dan Evaluasi Model :\n\nMelakukan pelatihan dan evaluasi performa masing-masing model.\nModel dilatih menggunakan data latih dan kemudian diuji menggunakan data uji.\nAkurasi dari setiap model dihitung menggunakan fungsi accuracy_score dan disimpan dalam dictionary accuracies.\n\nrumus yang digunakan di tahap ini :\n\\[\\text{Akurasi} = \\frac{\\text{Jumlah Prediksi Benar}}{\\text{Total Jumlah Prediksi}}\\]\n4. Pemilihan Model Terbaik : - Memilih model dengan akurasi tertinggi sebagai model terbaik. - Mencetak nama model terbaik dan akurasi yang terkait.\n5. Visualisasi Akurasi Model:  - Membuat diagram batang untuk membandingkan akurasi model. - Diagram batang menunjukkan akurasi setiap model pada sumbu y dan nama model pada sumbu x.\n\nwith open(\"scaler.pkl\", \"rb\") as scaler_file:\n    scaler = pickle.load(scaler_file)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\nmodels = {\n    'Logistic Regression': LogisticRegression(),\n    'KNN': KNeighborsClassifier(),\n    'SVM': SVC(),\n    'Decision Tree': DecisionTreeClassifier(),\n    'Random Forest': RandomForestClassifier()\n}\n\naccuracies = {}\nfor model_name, model in models.items():\n    model.fit(X_train, Y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(Y_test, y_pred)\n    accuracies[model_name] = accuracy\n\nbest_model = max(accuracies, key=accuracies.get)\nprint(f\"Metode Terbaik: {best_model}\")\nprint(f\"Akurasi Terbaik: {accuracies[best_model]}\")\n\nplt.bar(accuracies.keys(), accuracies.values())\nplt.xlabel('Metode Klasifikasi')\nplt.ylabel('Akurasi')\nplt.title('Perbandingan Akurasi Berbagai Metode Klasifikasi')\nplt.show()\n\nMetode Terbaik: Random Forest\nAkurasi Terbaik: 0.9608355091383812"
  },
  {
    "objectID": "OBESITAS.html#modelling-dengan-metode-random-forest",
    "href": "OBESITAS.html#modelling-dengan-metode-random-forest",
    "title": "1  ",
    "section": "1.16 3. Modelling Dengan Metode Random Forest",
    "text": "1.16 3. Modelling Dengan Metode Random Forest\nSetelah melalui proses pelatihan dan evaluasi berbagai metode klasifikasi, metode terbaik yang dipilih untuk memodelkan hubungan antara fitur dan target adalah Random Forest.\nRandom Forest adalah sebuah metode ensemble learning yang digunakan dalam pemodelan prediktif dan klasifikasi. Ensemble learning melibatkan penggabungan hasil dari beberapa model untuk meningkatkan kinerja dan akurasi keseluruhan. Random Forest khususnya menggunakan pohon keputusan sebagai model dasar dan menggabungkan prediksi dari beberapa pohon keputusan untuk membuat keputusan akhir.\nRandom Forest efektif untuk berbagai jenis tugas seperti klasifikasi dan regresi, dan sering digunakan karena kemampuannya yang baik dalam menangani data yang kompleks dan beragam. Keunggulan utamanya termasuk kemampuan untuk mengatasi overfitting, memberikan nilai penting untuk fitur, dan memberikan performa yang baik secara umum.\nBerikut adalah contoh sederhana bagaimana kita bisa menghitung prediksi menggunakan Random Forest dengan tiga pohon:\nMisalkan kita memiliki tiga pohon keputusan yang masing-masing memberikan prediksi sebagai berikut untuk suatu titik data:\n| Pohon | Prediksi |\n|-------|----------|\n| 1     | A        |\n| 2     | B        |\n| 3     | A        |\nDalam hal ini, Random Forest akan memberikan prediksi akhir “Kelas A” karena ini adalah kelas yang paling sering diprediksi oleh pohon-pohon dalam hutan.\nContoh Penerapan\nMisalkan kita memiliki data kategori obesitas dengan fitur seperti Usia, Tinggi (TB), dan Berat Badan (BB), dan target kita adalah kategori obesitas. Kita bisa menggunakan Random Forest untuk memprediksi jumlah unit yang akan diproduksi berdasarkan fitur-fitur tersebut.\n1. Persiapan Data\n| Usia | Tinggi Badan | Berat Badan | Target Kategori|\n|------|--------------|-------------|----------------|\n| 25   | 170          | 70          | Normal         |\n| 30   | 170          | 60          | Normal         |\n| 35   | 180          | 80          | Obesitas       |\n2. Pelatihan Model\nKedua, kita melatih model Random Forest menggunakan data historis tersebut. Model akan mempelajari hubungan kompleks antara “Usia”, “Tinggi Badan”, “Berat Badan”, dan target “Kategori Obesitas”.\n3. Penggunaan Model Untuk Prediksi\nSetelah melatih model, kita dapat menggunakannya untuk memprediksi kategori obesitas berdasarkan fitur-fitur baru. Misalkan kita memiliki data baru seperti berikut:\n| Usia | Tinggi Badan | Berat Badan |\n|------|--------------|-------------|\n| 28   | 175          | 75          |\n4. Hasil Prediksi Model kita kemudian memberikan prediksi kategori obesitas berikut:\n| Usia | Tinggi Badan | Berat Badan | Target Kategori|\n|------|--------------|-------------|----------------|\n| 28   | 175          | 75          | Obesitas       |\nDalam contoh ini, model Random Forest memprediksi bahwa dengan usia 28 tahun, tinggi badan 175 cm, dan berat badan 75 kg, seseorang termasuk dalam kategori “Obesitas”.\nPenting untuk dicatat bahwa ini adalah contoh sederhana, dan dalam praktiknya, validasi silang dan penyetelan parameter model tetap penting untuk memastikan kehandalan model pada data yang belum pernah dilihat sebelumnya. Selain itu, pemilihan target yang relevan dengan masalah kesehatan seperti obesitas memerlukan pemahaman domain yang baik.\nBerikut Langkah - langkah dalam Code :\n1. Pemulihan Model Scaler: - Model scaler (MinMaxScaler) yang telah disimpan sebelumnya dibaca kembali.\n2. Normalisasi Fitur: - Fitur-fitur dinormalisasi menggunakan model scaler.\n3. Iterasi untuk Memilih Model Terbaik: - Dilakukan iterasi untuk setiap nilai (n_estimators) dari 1 hingga 100. - Sebuah model Random Forest dibangun dengan jumlah pohon ((n_estimators)) tertentu. - Model tersebut dilatih pada data yang dinormalisasi. - Hasil prediksi dibandingkan dengan label sebenarnya untuk menghitung akurasi. - Jika akurasi model saat ini lebih baik daripada yang sebelumnya terbaik, model tersebut dianggap sebagai model terbaik.\n4. Penyimpanan Model Terbaik:\n\nModel Random Forest dengan akurasi tertinggi selama iterasi disimpan ke dalam file ‘best_rf_model.pkl’ menggunakan modul pickle.\n\nKode ini bertujuan untuk mencari model Random Forest dengan jumlah pohon terbaik yang memberikan akurasi tertinggi pada data yang telah diimbangi dan dinormalisasi. Model terbaik tersebut kemudian disimpan untuk penggunaan lebih lanjut.\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport pickle\n\ndata = data.copy()\n# memisahkan kolom fitur dan target\nX = data.drop(columns=['NObeyesdad','FAVC','CAEC','SMOKE','SCC','CALC','MTRANS','family_history_with_overweight','Gender'], axis =1)\nY = data['NObeyesdad']\n\nwith open('scaler.pkl', 'rb') as file:\n    scaler = pickle.load(file)\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport pickle\n\nbest_accuracy = 0.0\nbest_rf_model = None\nbest_n_estimators = 0\n\nfor n_estimators in range(1, 101):\n    rf_model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n    rf_model.fit(X, Y)\n\n    y_pred = rf_model.predict(X)\n    accuracy = accuracy_score(Y, y_pred)\n\n    print(f\"n_estimators = {n_estimators}, Akurasi: {accuracy:.2%}\")\n\n    if accuracy &gt; best_accuracy:\n        best_accuracy = accuracy\n        best_rf_model = rf_model\n        best_n_estimators = n_estimators\n\nprint(f\"\\nAkurasi terbaik: {best_accuracy:.2%} dengan n_estimators: {best_n_estimators}.\")\n\nwith open('best_rf_model.pkl', 'wb') as file:\n    pickle.dump(best_rf_model, file)\n\nn_estimators = 1, Akurasi: 95.09%\nn_estimators = 2, Akurasi: 95.14%\nn_estimators = 3, Akurasi: 98.90%\nn_estimators = 4, Akurasi: 98.80%\nn_estimators = 5, Akurasi: 99.27%\nn_estimators = 6, Akurasi: 99.48%\nn_estimators = 7, Akurasi: 99.58%\nn_estimators = 8, Akurasi: 99.69%\nn_estimators = 9, Akurasi: 99.69%\nn_estimators = 10, Akurasi: 99.74%\nn_estimators = 11, Akurasi: 99.84%\nn_estimators = 12, Akurasi: 99.84%\nn_estimators = 13, Akurasi: 99.79%\nn_estimators = 14, Akurasi: 99.90%\nn_estimators = 15, Akurasi: 99.90%\nn_estimators = 16, Akurasi: 99.95%\nn_estimators = 17, Akurasi: 99.95%\nn_estimators = 18, Akurasi: 99.95%\nn_estimators = 19, Akurasi: 99.90%\nn_estimators = 20, Akurasi: 99.95%\nn_estimators = 21, Akurasi: 99.95%\nn_estimators = 22, Akurasi: 99.95%\nn_estimators = 23, Akurasi: 99.95%\nn_estimators = 24, Akurasi: 99.95%\nn_estimators = 25, Akurasi: 99.95%\nn_estimators = 26, Akurasi: 99.95%\nn_estimators = 27, Akurasi: 99.95%\nn_estimators = 28, Akurasi: 99.95%\nn_estimators = 29, Akurasi: 99.95%\nn_estimators = 30, Akurasi: 99.95%\nn_estimators = 31, Akurasi: 100.00%\nn_estimators = 32, Akurasi: 100.00%\nn_estimators = 33, Akurasi: 99.95%\nn_estimators = 34, Akurasi: 99.95%\nn_estimators = 35, Akurasi: 99.95%\nn_estimators = 36, Akurasi: 100.00%\nn_estimators = 37, Akurasi: 99.95%\nn_estimators = 38, Akurasi: 100.00%\nn_estimators = 39, Akurasi: 100.00%\nn_estimators = 40, Akurasi: 100.00%\nn_estimators = 41, Akurasi: 100.00%\nn_estimators = 42, Akurasi: 100.00%\nn_estimators = 43, Akurasi: 100.00%\nn_estimators = 44, Akurasi: 100.00%\nn_estimators = 45, Akurasi: 100.00%\nn_estimators = 46, Akurasi: 100.00%\nn_estimators = 47, Akurasi: 100.00%\nn_estimators = 48, Akurasi: 100.00%\nn_estimators = 49, Akurasi: 100.00%\nn_estimators = 50, Akurasi: 100.00%\nn_estimators = 51, Akurasi: 100.00%\nn_estimators = 52, Akurasi: 100.00%\nn_estimators = 53, Akurasi: 100.00%\nn_estimators = 54, Akurasi: 100.00%\nn_estimators = 55, Akurasi: 100.00%\nn_estimators = 56, Akurasi: 100.00%\nn_estimators = 57, Akurasi: 100.00%\nn_estimators = 58, Akurasi: 100.00%\nn_estimators = 59, Akurasi: 100.00%\nn_estimators = 60, Akurasi: 100.00%\nn_estimators = 61, Akurasi: 100.00%\nn_estimators = 62, Akurasi: 100.00%\nn_estimators = 63, Akurasi: 100.00%\nn_estimators = 64, Akurasi: 100.00%\nn_estimators = 65, Akurasi: 100.00%\nn_estimators = 66, Akurasi: 100.00%\nn_estimators = 67, Akurasi: 100.00%\nn_estimators = 68, Akurasi: 100.00%\nn_estimators = 69, Akurasi: 100.00%\nn_estimators = 70, Akurasi: 100.00%\nn_estimators = 71, Akurasi: 100.00%\nn_estimators = 72, Akurasi: 100.00%\nn_estimators = 73, Akurasi: 100.00%\nn_estimators = 74, Akurasi: 100.00%\nn_estimators = 75, Akurasi: 100.00%\nn_estimators = 76, Akurasi: 100.00%\nn_estimators = 77, Akurasi: 100.00%\nn_estimators = 78, Akurasi: 100.00%\nn_estimators = 79, Akurasi: 100.00%\nn_estimators = 80, Akurasi: 100.00%\nn_estimators = 81, Akurasi: 100.00%\nn_estimators = 82, Akurasi: 100.00%\nn_estimators = 83, Akurasi: 100.00%\nn_estimators = 84, Akurasi: 100.00%\nn_estimators = 85, Akurasi: 100.00%\nn_estimators = 86, Akurasi: 100.00%\nn_estimators = 87, Akurasi: 100.00%\nn_estimators = 88, Akurasi: 100.00%\nn_estimators = 89, Akurasi: 100.00%\nn_estimators = 90, Akurasi: 100.00%\nn_estimators = 91, Akurasi: 100.00%\nn_estimators = 92, Akurasi: 100.00%\nn_estimators = 93, Akurasi: 100.00%\nn_estimators = 94, Akurasi: 100.00%\nn_estimators = 95, Akurasi: 100.00%\nn_estimators = 96, Akurasi: 100.00%\nn_estimators = 97, Akurasi: 100.00%\nn_estimators = 98, Akurasi: 100.00%\nn_estimators = 99, Akurasi: 100.00%\nn_estimators = 100, Akurasi: 100.00%\n\nAkurasi terbaik: 100.00% dengan n_estimators: 31.\n\n\n#\n\n\n\n\nEvaluasi - - -\n\n\n\n\nEvaluasi dalam konteks model prediktif dan klasifikasi merujuk pada penilaian kinerja model terhadap data yang belum pernah dilihat sebelumnya atau data uji. Tujuan evaluasi adalah untuk memahami sejauh mana model dapat menggeneralis ke data baru dan seberapa baik model dapat melakukan tugasnya, seperti klasifikasi dengan akurasi tinggi atau regresi dengan presisi yang baik. Berikut adalah beberapa metrik evaluasi umum:\n1. Akurasi (Accuracy): - Formula: \\[\\text{Accuracy} = \\frac{\\text{True Positive} + \\text{True Negative}}{\\text{Total Data}}\\] - Deskripsi:\n Akurasi mengukur sejauh mana model dapat mengklasifikasikan data dengan benar. Akurasi dinyatakan sebagai persentase dari total prediksi yang benar.\n2. Recall (Sensitivitas atau True Positive Rate): - Formula: \\[\\text{Recall} = \\frac{\\text{True Positive}}{\\text{True Positive} + \\text{False Negative}}\\] - Deskripsi:\n Recall mengukur sejauh mana model dapat mengidentifikasi semua instance yang benar positif dari kelas tertentu. Recall berguna ketika menghindari false negatives sangat penting.\n3. Precision: - Formula: \\[\\text{Precision} = \\frac{\\text{True Positive}}{\\text{True Positive} + \\text{False Positive}}\\] - Deskripsi:\n Precision mengukur sejauh mana prediksi positif model yang benar-benar benar. Precision berguna ketika menghindari false positives sangat penting.\n4. F1 Score: - Formula: \\[\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\] - Deskripsi:\n F1 Score adalah harmonic mean dari precision dan recall. Ini memberikan keseimbangan antara precision dan recall. F1 Score tinggi menunjukkan keseimbangan yang baik antara precision dan recall.\n5. Classification Report: - Menyajikan statistik klasifikasi seperti precision, recall, dan f1-score untuk setiap kelas. - Dalam classification report, Anda akan mendapatkan informasi ini untuk setiap kelas.\n\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n\n# data = pd.read_csv('new_preprocessing_obesitas.csv')\n\nX = data.drop(columns=['NObeyesdad','FAVC','CAEC','SMOKE','SCC','CALC','MTRANS','family_history_with_overweight','Gender'], axis =1)\nY = data['NObeyesdad']\n\nwith open('scaler.pkl', 'rb') as file:\n    scaler = pickle.load(file)\n\nwith open('best_rf_model.pkl', 'rb') as file:\n    best_rf_model = pickle.load(file)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\ny_pred = best_rf_model.predict(X_test)\n\naccuracy = accuracy_score(Y_test, y_pred)\nrecall = recall_score(Y_test, y_pred, average='weighted')\nprecision = precision_score(Y_test, y_pred, average='weighted')\nf1 = f1_score(Y_test, y_pred, average='weighted')\n\nprint(f'Akurasi: {accuracy:.2%}')\nprint(f'Recall: {recall:.2%}')\nprint(f'Precision: {precision:.2%}')\nprint(f'F1 Score: {f1:.2%}')\n\nprint('\\nClassification Report:')\nprint(classification_report(Y_test, y_pred))\n\nAkurasi: 100.00%\nRecall: 100.00%\nPrecision: 100.00%\nF1 Score: 100.00%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        61\n           1       1.00      1.00      1.00        48\n           2       1.00      1.00      1.00        54\n           3       1.00      1.00      1.00        44\n           4       1.00      1.00      1.00        57\n           5       1.00      1.00      1.00        60\n           6       1.00      1.00      1.00        59\n\n    accuracy                           1.00       383\n   macro avg       1.00      1.00      1.00       383\nweighted avg       1.00      1.00      1.00       383"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  }
]